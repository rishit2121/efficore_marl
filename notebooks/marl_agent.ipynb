{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Reinforcement Learning Implementation\n",
        "\n",
        "This notebook contains the implementation of the MARL agents used in the energy system. The agents include a base MARL agent and specialized agents for solar, grid, and battery control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Any, Tuple\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base MARL Agent\n",
        "\n",
        "The base MARL agent class implements the core functionality for all specialized agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MARLAgent(nn.Module):\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.config = config\n",
        "        self.observation_dim = config.get('observation_dim', 4)\n",
        "        self.action_dim = config.get('action_dim', 1)\n",
        "        self.hidden_dim = config.get('hidden_dim', 64)\n",
        "        self.time_step = 0\n",
        "        \n",
        "        # Policy network with continuous output\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(self.observation_dim + 3, self.hidden_dim),  # Added 3 for energy usage info\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, self.action_dim),\n",
        "            nn.Tanh()  # Output between -1 and 1\n",
        "        )\n",
        "        \n",
        "        # Value network\n",
        "        self.value_net = nn.Sequential(\n",
        "            nn.Linear(self.observation_dim + 3, self.hidden_dim),  # Added 3 for energy usage info\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "        # Learning parameters\n",
        "        self.base_lr = 0.1\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.base_lr, weight_decay=0.01)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        \n",
        "        # Energy constraints\n",
        "        self.max_daily_energy = 30.0  # kWh\n",
        "        self.current_daily_energy = 0.0\n",
        "        self.energy_usage = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        \n",
        "        # Learning parameters\n",
        "        self.target_energy = 30.0\n",
        "        self.energy_tolerance = 0.5\n",
        "        self.energy_penalty_scale = 1000.0\n",
        "        \n",
        "        # Track learning progress\n",
        "        self.energy_history = []\n",
        "        self.reward_history = []\n",
        "        self.best_reward = float('-inf')\n",
        "        self.patience = 2\n",
        "        self.patience_counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weight Initialization\n",
        "\n",
        "The `_init_weights` method initializes the neural network weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
        "        module.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Action Selection\n",
        "\n",
        "The `get_action` method selects actions based on observations and communication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action(self, observations: torch.Tensor, communication_channel=None) -> torch.Tensor:\n",
        "    if communication_channel is None:\n",
        "        communication_channel = {}\n",
        "        \n",
        "    # Share energy usage information\n",
        "    communication_channel['energy_usage'] = self.energy_usage\n",
        "    \n",
        "    # Get total energy usage from other agents\n",
        "    total_energy = sum(self.energy_usage.values())\n",
        "    energy_deviation = total_energy - self.target_energy\n",
        "    \n",
        "    # Add energy usage information to observations\n",
        "    energy_info = torch.tensor([\n",
        "        self.energy_usage['solar'],\n",
        "        self.energy_usage['grid'],\n",
        "        self.energy_usage['battery']\n",
        "    ], dtype=torch.float32)\n",
        "    \n",
        "    # Ensure observations are float32\n",
        "    observations = observations.to(torch.float32)\n",
        "    extended_obs = torch.cat([observations, energy_info])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        action = self.policy_net(extended_obs)\n",
        "        \n",
        "        # Add minimal noise for exploration\n",
        "        noise = torch.randn_like(action, dtype=torch.float32) * 0.01\n",
        "        action = torch.clamp(action + noise, -1, 1)\n",
        "        \n",
        "        # Calculate target step energy (30 kWh / 96 steps)\n",
        "        target_step_energy = 30.0 / 96.0\n",
        "        \n",
        "        # Predict the energy this action will produce\n",
        "        predicted_energy = action.abs() * self.max_daily_energy / 96.0  # Approximate scaling\n",
        "        \n",
        "        # Compute deviation just for this action\n",
        "        deviation = predicted_energy.sum().item() - target_step_energy\n",
        "        \n",
        "        if deviation > 0:\n",
        "            # If predicted energy too high, scale down slightly\n",
        "            scale_factor = max(0.5, 1 - 0.5 * (deviation / target_step_energy))\n",
        "            action = action * scale_factor\n",
        "        elif deviation < -0.1 * target_step_energy:\n",
        "            # If predicted energy too low, encourage scaling up\n",
        "            scale_factor = min(1.5, 1 + 0.5 * (-deviation / target_step_energy))\n",
        "            action = action * scale_factor\n",
        "        \n",
        "        action = torch.clamp(action, -1, 1)\n",
        "        \n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Update\n",
        "\n",
        "The `update_policy` method updates the agent's policy based on experience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_policy(self, observations: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor):\n",
        "    # Increment time step\n",
        "    self.time_step += 1\n",
        "    \n",
        "    # Store experience with energy usage information\n",
        "    energy_info = torch.tensor([\n",
        "        self.energy_usage['solar'],\n",
        "        self.energy_usage['grid'],\n",
        "        self.energy_usage['battery']\n",
        "    ], dtype=torch.float32)\n",
        "    \n",
        "    # Ensure all tensors are float32\n",
        "    observations = observations.to(torch.float32)\n",
        "    actions = actions.to(torch.float32)\n",
        "    rewards = rewards.to(torch.float32)\n",
        "    \n",
        "    extended_obs = torch.cat([observations, energy_info])\n",
        "    \n",
        "    self.buffer.append({\n",
        "        'observations': extended_obs,\n",
        "        'actions': actions,\n",
        "        'rewards': rewards\n",
        "    })\n",
        "    \n",
        "    if len(self.buffer) < self.batch_size:\n",
        "        return\n",
        "    \n",
        "    # Sample batch\n",
        "    batch = random.sample(self.buffer, self.batch_size)\n",
        "    obs_batch = torch.stack([x['observations'] for x in batch])\n",
        "    act_batch = torch.stack([x['actions'] for x in batch])\n",
        "    rew_batch = torch.stack([x['rewards'] for x in batch])\n",
        "    \n",
        "    # Calculate returns with extremely strong emphasis on negative rewards\n",
        "    returns = []\n",
        "    R = 0\n",
        "    for r in reversed(rew_batch):\n",
        "        if r < 0:\n",
        "            R = r + self.gamma * 0.01 * R  # Extremely strong discount for negative rewards\n",
        "        else:\n",
        "            R = r + self.gamma * 0.1 * R  # Very reduced retention of positive rewards\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns, dtype=torch.float32)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "    \n",
        "    # Calculate advantages with extremely strong emphasis on negative rewards\n",
        "    with torch.no_grad():\n",
        "        values = self.value_net(obs_batch)\n",
        "        advantages = returns - values.squeeze()\n",
        "        \n",
        "        # Scale advantages based on reward magnitude and energy usage\n",
        "        reward_magnitude = abs(rew_batch.mean().item())\n",
        "        energy_deviation = abs(sum(self.energy_usage.values()) - self.target_energy)\n",
        "        \n",
        "        # Extremely aggressive advantage scaling\n",
        "        if reward_magnitude > 100 or energy_deviation > 2:  # Bad state\n",
        "            advantage_scale = 1000.0  # Increased from 100.0\n",
        "        elif reward_magnitude > 50 or energy_deviation > 1:  # Moderately bad state\n",
        "            advantage_scale = 500.0  # Increased from 50.0\n",
        "        else:  # Near optimal state\n",
        "            advantage_scale = 1.0\n",
        "        \n",
        "        # Scale up advantages for negative rewards\n",
        "        negative_mask = rew_batch.squeeze() < 0\n",
        "        advantages[negative_mask] *= advantage_scale\n",
        "        \n",
        "        # Add immediate energy constraint penalty\n",
        "        if energy_deviation > self.energy_tolerance:\n",
        "            energy_penalty = -self.energy_penalty_scale * (energy_deviation - self.energy_tolerance)\n",
        "            advantages += energy_penalty\n",
        "    \n",
        "    # Update policy with more conservative learning\n",
        "    for _ in range(5):\n",
        "        # Calculate new action probabilities\n",
        "        new_actions = self.policy_net(obs_batch)\n",
        "        \n",
        "        # Calculate policy loss with stronger penalty for negative rewards\n",
        "        policy_loss = -torch.min(\n",
        "            (new_actions - act_batch).pow(2) * advantages.unsqueeze(-1),\n",
        "            torch.clamp(new_actions - act_batch, -0.1, 0.1).pow(2) * advantages.unsqueeze(-1)\n",
        "        ).mean()\n",
        "        \n",
        "        # Calculate value loss\n",
        "        value_loss = (self.value_net(obs_batch).squeeze() - returns).pow(2).mean()\n",
        "        \n",
        "        # Add immediate energy constraint loss\n",
        "        energy_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "        if sum(self.energy_usage.values()) > self.target_energy + self.energy_tolerance:\n",
        "            energy_loss = torch.tensor(self.energy_penalty_scale * (sum(self.energy_usage.values()) - self.target_energy), dtype=torch.float32)\n",
        "        \n",
        "        # Total loss with balanced weights\n",
        "        loss = policy_loss + 0.5 * value_loss + energy_loss\n",
        "        \n",
        "        # Update learning rate based on performance\n",
        "        current_reward = rewards.mean().item()\n",
        "        if current_reward < self.best_reward or energy_deviation > self.energy_tolerance:\n",
        "            self.patience_counter += 1\n",
        "            if self.patience_counter >= self.patience:\n",
        "                # Reduce learning rate more aggressively\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] *= 0.1\n",
        "                self.patience_counter = 0\n",
        "        else:\n",
        "            self.best_reward = current_reward\n",
        "            self.patience_counter = 0\n",
        "        \n",
        "        # Update with stronger gradient clipping\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), 0.1)  # More conservative gradient clipping\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    # Track progress\n",
        "    self.energy_history.append(sum(self.energy_usage.values()))\n",
        "    self.reward_history.append(rewards.mean().item())\n",
        "    \n",
        "    # Clear buffer\n",
        "    self.buffer = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Energy Management\n",
        "\n",
        "Methods for managing energy usage and tracking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_daily_energy(self, energy_used: float, source: str):\n",
        "    self.energy_usage[source] = energy_used\n",
        "    self.current_daily_energy = sum(self.energy_usage.values())\n",
        "\n",
        "def reset_daily_energy(self):\n",
        "    self.current_daily_energy = 0.0\n",
        "    self.energy_usage = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "    self.time_step = 0  # Reset time step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Persistence\n",
        "\n",
        "Methods for saving and loading the agent's state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save(self, path: str):\n",
        "    torch.save({\n",
        "        'model_state_dict': self.state_dict(),\n",
        "        'config': self.config,\n",
        "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        'energy_usage': self.energy_usage,\n",
        "        'current_daily_energy': self.current_daily_energy,\n",
        "        'time_step': self.time_step,\n",
        "        'energy_history': self.energy_history,\n",
        "        'reward_history': self.reward_history,\n",
        "        'best_reward': self.best_reward,\n",
        "        'patience_counter': self.patience_counter\n",
        "    }, path)\n",
        "\n",
        "def load(self, path: str):\n",
        "    checkpoint = torch.load(path)\n",
        "    self.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.config = checkpoint['config']\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    self.energy_usage = checkpoint['energy_usage']\n",
        "    self.current_daily_energy = checkpoint['current_daily_energy']\n",
        "    self.time_step = checkpoint['time_step']\n",
        "    self.energy_history = checkpoint['energy_history']\n",
        "    self.reward_history = checkpoint['reward_history']\n",
        "    self.best_reward = checkpoint['best_reward']\n",
        "    self.patience_counter = checkpoint['patience_counter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solar Agent\n",
        "\n",
        "The solar agent specializes in managing solar energy generation and usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SolarAgent(MARLAgent):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.max_power = 5.0  # Maximum solar power in kW\n",
        "        self.efficiency = 0.9  # Solar panel efficiency\n",
        "        self.base_action_scale = 0.5  # Base scaling for solar actions\n",
        "\n",
        "    def get_action(self, observations: torch.Tensor, communication_channel=None) -> torch.Tensor:\n",
        "        if communication_channel is None:\n",
        "            communication_channel = {}\n",
        "            \n",
        "        # Get current state information\n",
        "        time_step = int(observations[3].item())\n",
        "        current_hour = (time_step // 4) % 24\n",
        "        solar_potential = self._get_solar_potential(current_hour)\n",
        "        \n",
        "        # Share solar information\n",
        "        communication_channel['solar_agent'] = {\n",
        "            'solar_available': solar_potential,\n",
        "            'current_generation': self.energy_usage['solar']\n",
        "        }\n",
        "        \n",
        "        # Get base action from policy network\n",
        "        base_action = super().get_action(observations, communication_channel)\n",
        "        \n",
        "        # Scale action based on solar potential and time of day\n",
        "        if solar_potential > 0:\n",
        "            # During peak solar hours (9 AM - 4 PM), maximize solar usage\n",
        "            if 9 <= current_hour < 16:\n",
        "                scaled_action = torch.clamp(base_action + 0.8, -1, 1)  # Strong bias towards using solar\n",
        "            else:\n",
        "                scaled_action = torch.clamp(base_action + 0.5, -1, 1)  # Moderate bias\n",
        "            \n",
        "            # Scale by solar potential and base action scale\n",
        "            scaled_action = scaled_action * (solar_potential / self.max_power) * self.base_action_scale\n",
        "        else:\n",
        "            scaled_action = torch.zeros_like(base_action)\n",
        "        \n",
        "        return scaled_action\n",
        "\n",
        "    def _get_solar_potential(self, hour: int) -> float:\n",
        "        if 5 <= hour < 19:  # Solar hours (5 AM to 7 PM)\n",
        "            position = (hour - 5) / 14\n",
        "            return self.max_power * np.exp(-((position - 0.5) ** 2) / 0.1)\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Agent\n",
        "\n",
        "The grid agent manages grid power usage and pricing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridAgent(MARLAgent):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.peak_price = 0.58672\n",
        "        self.off_peak_price = 0.46366\n",
        "        self.peak_hours = set(range(7, 23))  # Peak hours from 7 AM to 11 PM\n",
        "        self.base_action_scale = 0.3  # Base scaling for grid actions\n",
        "\n",
        "    def get_action(self, observations: torch.Tensor, communication_channel=None) -> torch.Tensor:\n",
        "        if communication_channel is None:\n",
        "            communication_channel = {}\n",
        "            \n",
        "        # Get current state information\n",
        "        time_step = int(observations[3].item())\n",
        "        current_hour = (time_step // 4) % 24\n",
        "        grid_price = observations[1].item()\n",
        "        \n",
        "        # Get info from other agents\n",
        "        solar_info = communication_channel.get('solar_agent', {})\n",
        "        battery_info = communication_channel.get('battery_agent', {})\n",
        "        \n",
        "        # Calculate remaining demand after solar and battery\n",
        "        solar_available = solar_info.get('solar_available', 0)\n",
        "        battery_discharge = max(0, battery_info.get('current_charge', 0) - battery_info.get('min_reserve', 2.5))\n",
        "        \n",
        "        # Get base action from policy network\n",
        "        base_action = super().get_action(observations, communication_channel)\n",
        "        \n",
        "        # Scale action based on time of day and energy usage\n",
        "        if current_hour in self.peak_hours:\n",
        "            scaled_action = torch.clamp(base_action - 0.5, -1, 1) * self.base_action_scale  # Minimize grid usage during peak\n",
        "        else:\n",
        "            scaled_action = torch.clamp(base_action + 0.2, -1, 1) * self.base_action_scale  # Allow more grid usage during off-peak\n",
        "        \n",
        "        return scaled_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Battery Agent\n",
        "\n",
        "The battery agent manages battery charging and discharging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BatteryAgent(MARLAgent):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.battery_capacity = 12.5  # 12.5 kWh capacity\n",
        "        self.battery_charge = self.battery_capacity * 0.6  # Start at 60% charge\n",
        "        self.min_reserve = 2.5        # Minimum 2.5 kWh reserve for emergencies\n",
        "        self.charge_threshold = 0.6   # Target to maintain 60% charge when possible\n",
        "        self.power_rating = 5.0       # Maximum charge/discharge rate in kW\n",
        "        self.efficiency = 0.9         # Charging/discharging efficiency\n",
        "        self.base_action_scale = 0.4  # Base scaling for battery actions\n",
        "\n",
        "    def get_action(self, observations: torch.Tensor, communication_channel=None) -> torch.Tensor:\n",
        "        if communication_channel is None:\n",
        "            communication_channel = {}\n",
        "            \n",
        "        # Get current battery state\n",
        "        battery_state = observations[3].item()\n",
        "        self.battery_charge = battery_state\n",
        "        available_capacity = self.battery_capacity - battery_state\n",
        "        grid_price = observations[1].item()\n",
        "        time_step = int(observations[3].item())\n",
        "        current_hour = (time_step // 4) % 24\n",
        "\n",
        "        # Share battery status\n",
        "        communication_channel['battery_agent'] = {\n",
        "            'current_charge': battery_state,\n",
        "            'available_capacity': available_capacity,\n",
        "            'min_reserve': self.min_reserve\n",
        "        }\n",
        "        \n",
        "        # Get info from other agents\n",
        "        solar_info = communication_channel.get('solar_agent', {})\n",
        "        solar_available = solar_info.get('solar_available', 0)\n",
        "        \n",
        "        # Get base action from policy network\n",
        "        base_action = super().get_action(observations, communication_channel)\n",
        "        \n",
        "        # Scale action based on battery state and time of day\n",
        "        if self.battery_charge < self.min_reserve:\n",
        "            # Emergency charging needed\n",
        "            if solar_available > 0:\n",
        "                scaled_action = torch.clamp(base_action + 0.9, -1, 1) * self.base_action_scale  # Strong bias towards charging from solar\n",
        "            else:\n",
        "                scaled_action = torch.clamp(base_action + 0.7, -1, 1) * self.base_action_scale  # Charge from grid if needed\n",
        "        elif current_hour in range(7, 23):  # Peak hours\n",
        "            if self.battery_charge > self.min_reserve * 1.5:\n",
        "                scaled_action = torch.clamp(base_action - 0.8, -1, 1) * self.base_action_scale  # Bias towards discharging\n",
        "            elif solar_available > 0 and self.battery_charge < self.battery_capacity * 0.8:\n",
        "                scaled_action = torch.clamp(base_action + 0.6, -1, 1) * self.base_action_scale  # Charge from solar if available\n",
        "            else:\n",
        "                scaled_action = base_action * self.base_action_scale\n",
        "        else:  # Off-peak hours\n",
        "            if self.battery_charge < self.battery_capacity * 0.7:\n",
        "                scaled_action = torch.clamp(base_action + 0.5, -1, 1) * self.base_action_scale  # Charge from grid\n",
        "            elif solar_available > 0 and grid_price > 0.4:\n",
        "                scaled_action = torch.clamp(base_action + 0.4, -1, 1) * self.base_action_scale  # Charge from solar if cheaper\n",
        "            else:\n",
        "                scaled_action = base_action * self.base_action_scale\n",
        "        \n",
        "        return scaled_action\n",
        "\n",
        "    def _calculate_action_utilities(self, state):\n",
        "        current_time = state['time_step'][0]\n",
        "        current_part = ['morning', 'afternoon', 'evening'][int(current_time) % 3]\n",
        "        grid_price = state['grid_pricing'][0]\n",
        "        demand = state['household_demand'][0]\n",
        "\n",
        "        # Solar utility calculation - make it very attractive during high-price periods\n",
        "        solar_utility = 1.0  # Base utility\n",
        "        if current_part == 'morning':\n",
        "            solar_utility *= 1.5  # Higher utility in morning\n",
        "        elif current_part == 'afternoon':\n",
        "            solar_utility *= 2.0  # Highest utility in afternoon\n",
        "        elif current_part == 'evening':\n",
        "            solar_utility *= 1.2  # Still useful but less available\n",
        "\n",
        "        # Grid utility calculation - inverse to price\n",
        "        grid_utility = 1.0 / (grid_price + 0.1)  # Add 0.1 to avoid division by zero\n",
        "\n",
        "        # Battery utility based on charge level and time of day\n",
        "        battery_charge = state.get('battery_level', [0.5])[0]  # Default to 50% if not available\n",
        "        \n",
        "        # Battery is more valuable during evening (high price) period\n",
        "        battery_utility = 0.5\n",
        "        if current_part == 'evening':\n",
        "            battery_utility = 1.5 if battery_charge > 0.3 else 0.2  # High utility if charged\n",
        "        elif current_part == 'morning':\n",
        "            battery_utility = 0.3 if battery_charge < 0.8 else 0.1  # Prefer charging\n",
        "        else:  # afternoon\n",
        "            battery_utility = 0.8 if battery_charge < 0.9 else 0.2  # Balance charging/discharging\n",
        "\n",
        "        return solar_utility, grid_utility, battery_utility"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
