{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Reinforcement Learning Training\n",
        "\n",
        "This notebook contains the training implementation for the MARL agents in the energy system. It includes functions for loading configuration, setting up logging, managing checkpoints, and the main training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "from environment.energy_env import EnergyResilienceEnv\n",
        "from agents.marl_agent import MARLAgent, SolarAgent, GridAgent, BatteryAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Loading\n",
        "\n",
        "Function to load training configuration from YAML file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_config(config_path: str) -> Dict[str, Any]:\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging Setup\n",
        "\n",
        "Function to initialize Weights & Biases logging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_logging(config: Dict[str, Any]) -> None:\n",
        "    if config['logging']['wandb']:\n",
        "        wandb.init(\n",
        "            project=config['logging']['wandb_project'],\n",
        "            entity=config['logging']['wandb_entity'],\n",
        "            config=config\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint Management\n",
        "\n",
        "Functions for saving and loading model checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(agents, env, episode, save_dir='checkpoints'):\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Create save directory if it doesn't exist\n",
        "    save_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), save_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Create a timestamped directory for this checkpoint\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    checkpoint_dir = os.path.join(save_dir, f'checkpoint_ep{episode}_{timestamp}')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    # Save each agent's state\n",
        "    for i, agent in enumerate(agents):\n",
        "        agent_path = os.path.join(checkpoint_dir, f'agent_{i}.pt')\n",
        "        agent.save(agent_path)\n",
        "    \n",
        "    # Save environment state\n",
        "    env_state = {\n",
        "        'battery_charge': env.battery_charge,\n",
        "        'daily_solar_generated': env.daily_solar_generated,\n",
        "        'solar_energy_storage': env.solar_energy_storage,\n",
        "        'available_solar': env.available_solar,\n",
        "        'time_step': env.time_step,\n",
        "        'metrics': env.metrics\n",
        "    }\n",
        "    torch.save(env_state, os.path.join(checkpoint_dir, 'env_state.pt'))\n",
        "    \n",
        "    print(f\"\\nCheckpoint saved at episode {episode} to {checkpoint_dir}\")\n",
        "    return checkpoint_dir\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, config):\n",
        "    import os\n",
        "    \n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
        "    \n",
        "    # Load agents\n",
        "    agents = []\n",
        "    agent_types = [SolarAgent, GridAgent, BatteryAgent]\n",
        "    \n",
        "    for i, agent_type in enumerate(agent_types):\n",
        "        agent_path = os.path.join(checkpoint_dir, f'agent_{i}.pt')\n",
        "        if not os.path.exists(agent_path):\n",
        "            raise ValueError(f\"Agent checkpoint {agent_path} not found\")\n",
        "        \n",
        "        agent = agent_type(config['network'])\n",
        "        agent.load(agent_path)\n",
        "        agents.append(agent)\n",
        "    \n",
        "    # Load environment state\n",
        "    env = EnergyResilienceEnv(config['environment'])\n",
        "    env_state_path = os.path.join(checkpoint_dir, 'env_state.pt')\n",
        "    if os.path.exists(env_state_path):\n",
        "        env_state = torch.load(env_state_path)\n",
        "        env.battery_charge = env_state['battery_charge']\n",
        "        env.daily_solar_generated = env_state['daily_solar_generated']\n",
        "        env.solar_energy_storage = env_state['solar_energy_storage']\n",
        "        env.available_solar = env_state['available_solar']\n",
        "        env.time_step = env_state['time_step']\n",
        "        env.metrics = env_state['metrics']\n",
        "    \n",
        "    print(f\"\\nCheckpoint loaded from {checkpoint_dir}\")\n",
        "    return agents, env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "The main training function that runs the MARL training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config: Dict[str, Any], load_checkpoint_dir: str = None) -> None:\n",
        "    # Setup environment and agents\n",
        "    if load_checkpoint_dir:\n",
        "        print(f\"Loading checkpoint from {load_checkpoint_dir}\")\n",
        "        agents, env = load_checkpoint(load_checkpoint_dir, config)\n",
        "    else: \n",
        "        env = EnergyResilienceEnv(config['environment'])\n",
        "        agents = [\n",
        "            SolarAgent(config['network']),\n",
        "            GridAgent(config['network']),\n",
        "            BatteryAgent(config['network'])\n",
        "        ]\n",
        "\n",
        "    # Initialize communication channel with default values\n",
        "    communication_channel = {\n",
        "        'solar_agent': {\n",
        "            'solar_available': 0,\n",
        "            'current_generation': 0\n",
        "        },\n",
        "        'grid_agent': {\n",
        "            'grid_pricing': 0.4,  # Default to morning price\n",
        "            'current_demand': 0.5  # Increased from 1.5 to 3.0 kW\n",
        "        },\n",
        "        'battery_agent': {\n",
        "            'current_charge': 0,\n",
        "            'available_capacity': 12.5,  # Full battery capacity\n",
        "            'min_reserve': 2.5,\n",
        "            'investment_cost': 0.001\n",
        "        },\n",
        "        'energy_usage': {\n",
        "            'solar': 0.0,\n",
        "            'grid': 0.0,\n",
        "            'battery': 0.0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for episode in range(config['training']['num_episodes']):\n",
        "        # Reset communication channel at start of episode\n",
        "        communication_channel.update({\n",
        "            'solar_agent': {\n",
        "                'solar_available': 5.0,  # Increased from 0 to 5 kW\n",
        "                'current_generation': 0\n",
        "            },\n",
        "            'grid_agent': {\n",
        "                'grid_pricing': 0.4,\n",
        "                'current_demand': 3.0  # Increased from 1.5 to 3.0 kW\n",
        "            },\n",
        "            'battery_agent': {\n",
        "                'current_charge': 6.25,  # Start at 50% capacity instead of 0\n",
        "                'available_capacity': 12.5,\n",
        "                'min_reserve': 2.5,\n",
        "                'investment_cost': 0.001\n",
        "            },\n",
        "            'energy_usage': {\n",
        "                'solar': 0.0,\n",
        "                'grid': 0.0,\n",
        "                'battery': 0.0\n",
        "            }\n",
        "        })\n",
        "\n",
        "        initial_observations, _ = env.reset()\n",
        "        \n",
        "        # Reset daily energy tracking for all agents\n",
        "        for agent in agents:\n",
        "            agent.reset_daily_energy()\n",
        "\n",
        "        # Format observations for each agent with float32 type\n",
        "        observations = {\n",
        "            f'agent_{i}': torch.FloatTensor([\n",
        "                initial_observations['solar_energy_storage'][0],\n",
        "                initial_observations['grid_pricing'][0],\n",
        "                initial_observations['household_demand'][0],\n",
        "                env.battery_charge  # Add battery state as fourth observation\n",
        "            ])\n",
        "            for i in range(config['environment']['n_agents'])\n",
        "        }\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Initialize session tracking\n",
        "        morning_energy = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        afternoon_energy = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        evening_energy = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        \n",
        "        morning_costs = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        afternoon_costs = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "        evening_costs = {'solar': 0.0, 'grid': 0.0, 'battery': 0.0}\n",
        "\n",
        "        for step in range(env.max_steps):\n",
        "            actions = []\n",
        "            # Get actions from each agent with communication\n",
        "            for i, agent in enumerate(agents):\n",
        "                obs_tensor = torch.FloatTensor(observations[f'agent_{i}'])  # Ensure float32 type\n",
        "                action = agent.get_action(obs_tensor, communication_channel)\n",
        "                \n",
        "                # Convert action to scalar value regardless of source type\n",
        "                if isinstance(action, torch.Tensor):\n",
        "                    action = action.item()  # Convert single tensor value to scalar\n",
        "                elif isinstance(action, np.ndarray):\n",
        "                    action = float(action.flatten()[0])  # Convert first element to scalar\n",
        "                elif isinstance(action, (np.float32, np.float64)):\n",
        "                    action = float(action)\n",
        "                    \n",
        "                actions.append(action)\n",
        "\n",
        "            # Step the environment with the list of actions\n",
        "            next_observations, reward, done, truncated, info = env.step(actions)\n",
        "            done = done or truncated\n",
        "            \n",
        "            # Track energy usage and costs by session\n",
        "            current_hour = (step // 4) % 24\n",
        "            power_usage = info['power_by_session']\n",
        "            costs = info['costs_by_session']\n",
        "            \n",
        "            if 6 <= current_hour < 9:  # Morning session\n",
        "                if power_usage['morning']:\n",
        "                    morning_energy['solar'] += power_usage['morning']['solar']\n",
        "                    morning_energy['grid'] += power_usage['morning']['grid']\n",
        "                    morning_energy['battery'] += abs(power_usage['morning']['battery'])\n",
        "                if costs['morning']:\n",
        "                    morning_costs['solar'] += costs['morning']['solar']\n",
        "                    morning_costs['grid'] += costs['morning']['grid']\n",
        "                    morning_costs['battery'] += costs['morning']['battery']\n",
        "            elif 9 <= current_hour < 16:  # Afternoon session\n",
        "                if power_usage['afternoon']:\n",
        "                    afternoon_energy['solar'] += power_usage['afternoon']['solar']\n",
        "                    afternoon_energy['grid'] += power_usage['afternoon']['grid']\n",
        "                    afternoon_energy['battery'] += abs(power_usage['afternoon']['battery'])\n",
        "                if costs['afternoon']:\n",
        "                    afternoon_costs['solar'] += costs['afternoon']['solar']\n",
        "                    afternoon_costs['grid'] += costs['afternoon']['grid']\n",
        "                    afternoon_costs['battery'] += costs['afternoon']['battery']\n",
        "            elif 16 <= current_hour < 21:  # Evening session\n",
        "                if power_usage['evening']:\n",
        "                    evening_energy['solar'] += power_usage['evening']['solar']\n",
        "                    evening_energy['grid'] += power_usage['evening']['grid']\n",
        "                    evening_energy['battery'] += abs(power_usage['evening']['battery'])\n",
        "                if costs['evening']:\n",
        "                    evening_costs['solar'] += costs['evening']['solar']\n",
        "                    evening_costs['grid'] += costs['evening']['grid']\n",
        "                    evening_costs['battery'] += costs['evening']['battery']\n",
        "            \n",
        "            # Update energy usage for each agent\n",
        "            total_solar = morning_energy['solar'] + afternoon_energy['solar'] + evening_energy['solar']\n",
        "            total_grid = morning_energy['grid'] + afternoon_energy['grid'] + evening_energy['grid']\n",
        "            total_battery = morning_energy['battery'] + afternoon_energy['battery'] + evening_energy['battery']\n",
        "            \n",
        "            # Update energy usage in communication channel\n",
        "            communication_channel['energy_usage'] = {\n",
        "                'solar': total_solar,\n",
        "                'grid': total_grid,\n",
        "                'battery': total_battery\n",
        "            }\n",
        "            \n",
        "            # Update each agent's energy tracking\n",
        "            agents[0].update_daily_energy(total_solar, 'solar')\n",
        "            agents[1].update_daily_energy(total_grid, 'grid')\n",
        "            agents[2].update_daily_energy(total_battery, 'battery')\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Update agents with stronger emphasis on negative rewards\n",
        "            for i, agent in enumerate(agents):\n",
        "                obs_tensor = torch.FloatTensor(observations[f'agent_{i}'])  # Ensure float32 type\n",
        "                # Convert single action value to tensor properly\n",
        "                action_value = actions[i]\n",
        "                if isinstance(action_value, (np.ndarray, np.float32, np.float64)):\n",
        "                    action_value = float(action_value)\n",
        "                action = torch.FloatTensor([action_value])  # Ensure float32 type\n",
        "                agent_reward = torch.FloatTensor([reward])  # Ensure float32 type\n",
        "                \n",
        "                # Scale up negative rewards\n",
        "                if reward < 0:\n",
        "                    agent_reward = agent_reward * 2.0\n",
        "                \n",
        "                agent.update_policy(obs_tensor, action, agent_reward)\n",
        "\n",
        "            # Update observations with float32 type\n",
        "            observations = {\n",
        "                f'agent_{i}': torch.FloatTensor([\n",
        "                    next_observations['solar_energy_storage'][0],\n",
        "                    next_observations['grid_pricing'][0],\n",
        "                    next_observations['household_demand'][0],\n",
        "                    env.battery_charge  # Add battery state as fourth observation\n",
        "                ])\n",
        "                for i in range(config['environment']['n_agents'])\n",
        "            }\n",
        "\n",
        "            if done:\n",
        "                # Calculate total daily cost\n",
        "                total_daily_cost = (\n",
        "                    sum(morning_costs.values()) + \n",
        "                    sum(afternoon_costs.values()) + \n",
        "                    sum(evening_costs.values())\n",
        "                )\n",
        "                \n",
        "                # Print total energy usage for the day\n",
        "                total_solar = morning_energy['solar'] + afternoon_energy['solar'] + evening_energy['solar']\n",
        "                total_grid = morning_energy['grid'] + afternoon_energy['grid'] + evening_energy['grid']\n",
        "                total_battery = morning_energy['battery'] + afternoon_energy['battery'] + evening_energy['battery']\n",
        "                total_energy = total_solar + total_grid + total_battery\n",
        "                \n",
        "                print(f\"\\nDay {episode + 1}\")\n",
        "                print(f\"Energy: {total_energy:.1f} kWh (Solar: {total_solar/total_energy*100:.0f}%, Grid: {total_grid/total_energy*100:.0f}%, Battery: {total_battery/total_energy*100:.0f}%)\")\n",
        "                print(f\"Cost: ${total_daily_cost:.2f} | Battery: {env.battery_charge:.1f} kWh\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "                break\n",
        "       \n",
        "        # Save model checkpoint periodically\n",
        "        if (episode + 1) % config['training'].get('save_interval', 100) == 0:\n",
        "            save_checkpoint(agents, env, episode + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Function to evaluate trained agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(env: EnergyResilienceEnv, agents: list, config: Dict[str, Any]) -> None:\n",
        "    eval_rewards = []\n",
        "    eval_lengths = []\n",
        "\n",
        "    for episode in range(1):  # only run 1 episode for debugging\n",
        "        try:\n",
        "            initial_observations, _ = env.reset()\n",
        "        except Exception as e:\n",
        "            print(\"üí• Error during env.reset():\", e)\n",
        "            return\n",
        "\n",
        "        # Format observations for each agent\n",
        "        observations = {\n",
        "            f'agent_{i}': np.array([\n",
        "                initial_observations['solar_energy_storage'][0],\n",
        "                initial_observations['grid_pricing'][0],\n",
        "                initial_observations['household_demand'][0],\n",
        "                initial_observations['time_step'][0]\n",
        "            ])\n",
        "            for i in range(config['environment']['n_agents'])\n",
        "        }\n",
        "\n",
        "        print(\"üîç Initial observations:\", observations.keys())\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        for step in range(5):  # only a few steps\n",
        "            actions = []\n",
        "            for i, agent in enumerate(agents):\n",
        "                key = f'agent_{i}'\n",
        "                if key not in observations:\n",
        "                    raise KeyError(f\"‚ùå Missing observation for {key}. Full observations: {observations}\")\n",
        "                obs_tensor = torch.FloatTensor(observations[key])\n",
        "                action = agent.get_action(obs_tensor, deterministic=True)\n",
        "                actions.append(action.numpy())\n",
        "\n",
        "            next_observations, rewards, done, truncated, info = env.step(actions)\n",
        "            episode_reward += np.mean(rewards)\n",
        "            episode_length += 1\n",
        "\n",
        "            # Reformat next_observations for each agent\n",
        "            observations = {\n",
        "                f'agent_{i}': np.array([\n",
        "                    next_observations['solar_energy_storage'][0],\n",
        "                    next_observations['grid_pricing'][0],\n",
        "                    next_observations['household_demand'][0],\n",
        "                    next_observations['time_step'][0]\n",
        "                ])\n",
        "                for i in range(config['environment']['n_agents'])\n",
        "            }\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        eval_rewards.append(episode_reward)\n",
        "        eval_lengths.append(episode_length)\n",
        "\n",
        "    print(f\"‚úÖ Eval result: mean reward = {np.mean(eval_rewards)}, mean length = {np.mean(eval_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution\n",
        "\n",
        "The main block that runs the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load configuration\n",
        "    config = load_config('configs/training_config.yaml')\n",
        "    \n",
        "    # Setup logging\n",
        "    setup_logging(config)\n",
        "    \n",
        "    # Start training\n",
        "    train(config)\n",
        "    \n",
        "    # Close wandb if used\n",
        "    if config['logging']['wandb']:\n",
        "        wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
